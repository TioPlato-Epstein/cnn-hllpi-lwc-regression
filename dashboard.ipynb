{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment dashboard"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  from google import colab\n",
    "  colab.drive.mount(\"/content/gdrive\")\n",
    "  %pip install -qU neptune\n",
    "  DRIVE_ROOT = \"/content/gdrive/My Drive/\"\n",
    "except ModuleNotFoundError:\n",
    "  DRIVE_ROOT = \"H:\\\\我的云端硬盘\\\\\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "PROJECT_NAME = \"cnn-hllpi-lwc-regression\"\n",
    "PROJECT_ROOT = os.path.join(DRIVE_ROOT, \"Repositories\", PROJECT_NAME)\n",
    "print(f\"Project root at:\\n  {PROJECT_ROOT}\")\n",
    "\n",
    "DATA_FOLDER = os.path.join(PROJECT_ROOT, \"data\")\n",
    "assert os.path.exists(DATA_FOLDER), \"Data folder not found!\"\n",
    "EXPERIMENT_FOLDER = os.path.join(PROJECT_ROOT, \"experiments\")\n",
    "os.makedirs(EXPERIMENT_FOLDER, exist_ok=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Config logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import neptune\n",
    "from neptune import types\n",
    "\n",
    "neptune_logger = neptune.init_run(\n",
    "  project=f\"tioplato-epstein/{PROJECT_NAME}\",\n",
    "  api_token=\"ANONYMOUS\",\n",
    "  # ID of debug run, change to None for new run or other ID for resume\n",
    "  with_id=\"CNNHLLPI-1\",\n",
    "  git_ref=types.GitRef(f\"{PROJECT_ROOT}\")\n",
    ")\n",
    "\n",
    "if neptune_logger._with_id is not None:\n",
    "  try:\n",
    "    del neptune_logger[\"metrics\"]\n",
    "    del neptune_logger[\"checkpoints\"]\n",
    "    del neptune_logger[\"configurations\"]\n",
    "    del neptune_logger[\"plots\"]\n",
    "  except BaseException:\n",
    "    pass  # Just to make sure that the keys are not there"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(PROJECT_ROOT)\n",
    "import utils\n",
    "from torchvision import transforms\n",
    "\n",
    "utils.ensure_reproduction(42)\n",
    "\n",
    "INPUT_IMAGE_SIZE = (512, 512)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "  transforms.RandomHorizontalFlip(),\n",
    "  transforms.RandomVerticalFlip(),\n",
    "  transforms.RandomRotation(180),\n",
    "  transforms.Resize(INPUT_IMAGE_SIZE),\n",
    "  transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "std, mean = utils.compute_std_mean(DATA_FOLDER, transform)\n",
    "print(f\"std: {std}\\nmean: {mean}\")\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "  transforms.RandomHorizontalFlip(),\n",
    "  transforms.RandomVerticalFlip(),\n",
    "  transforms.RandomRotation(180),\n",
    "  transforms.Resize(INPUT_IMAGE_SIZE),\n",
    "  transforms.ToTensor(),\n",
    "  transforms.Normalize(mean, std),\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "  transforms.Resize(INPUT_IMAGE_SIZE),\n",
    "  transforms.ToTensor(),\n",
    "  transforms.Normalize(mean, std),\n",
    "])\n",
    "\n",
    "dataloaders = utils.prepare_dataloaders(\n",
    "  DATA_FOLDER,\n",
    "  train_transform,\n",
    "  test_transform,\n",
    "  batch_sizes=[20, 5, 5],\n",
    "  num_workers=0,\n",
    "  train_split_ratio=0.8,\n",
    ")\n",
    "\n",
    "utils.check_dataloaders(dataloaders)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Run experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import models\n",
    "from train_evaluate import train_model, test_model\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "utils.register_model(\"alexnet\", models.AlexNet)\n",
    "train_result = train_model(\n",
    "  os.path.join(EXPERIMENT_FOLDER, \"alexnet\"),\n",
    "  \"config.json\",\n",
    "  dataloaders[\"train\"],\n",
    "  dataloaders[\"val\"],\n",
    "  device,\n",
    "  neptune_logger,\n",
    "  epochs=100,\n",
    "  val_every=5,\n",
    ")\n",
    "print(f\"Best val loss: {train_result['best_loss']}.\")\n",
    "\n",
    "test_result = test_model(\n",
    "  os.path.join(EXPERIMENT_FOLDER, \"alexnet\"),\n",
    "  \"config.json\",\n",
    "  train_result[\"best_model\"],\n",
    "  dataloaders[\"test\"],\n",
    "  device,\n",
    "  neptune_logger,\n",
    ")\n",
    "print(f\"Test loss: {test_result['loss']}, \"\n",
    "      f\"test accuracy: {test_result['accuracy']}.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Stop logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neptune_logger.wait()\n",
    "neptune_logger.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ipytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
